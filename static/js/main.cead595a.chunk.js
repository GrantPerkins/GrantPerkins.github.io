(this.webpackJsonpportfolio=this.webpackJsonpportfolio||[]).push([[0],{20:function(e){e.exports=JSON.parse('[{"name":"Pressure Injury Stage Image Classification using Cross-Shaped Window Attention Vision Transformers","link":"https://github.com/GrantPerkins/CSWin-Transformer","image":"cswin.png","description":"Pressure injuries affect up to three million people globally, leading to extended hospital stays and financial hardship for patients and their families. Proper treatment for a pressure injury depends on its stage (severity). Unfortunately, nurses have been found to be less than 70% accurate when diagnosing the stage of a pressure injury, resulting in improper treatment, and delaying the healing process for those misdiagnosed. To improve this situation, researchers have utilized deep learning models to classify the stage of a pressure injury from an image. Deep learning models struggle to learn from pressure injury image datasets for two reasons: limited samples and high intra-class variation with low inter-class variation. To address these challenges, we propose adapting the Cross-Shaped Window (CSWin) transformer to the task of pressure injury classification. Specifically, cross-shaped window self-attention decreases intra-class variation and increase inter-class variation more efficiently than previous approaches. We also utilize a model fine-tuning methodology to improve model robustness by pre-training the CSWin transformer on a larger and generalized dataset before fine-tuning it on our smaller pressure injury dataset, facilitating effective pressure injury stage classification from few samples. In rigorous evaluation, CSWin achieved an accuracy of 78.5%, outperforming the accuracy of prior state-of-the-art approaches by more than 8%. Deployment of the CSWin transformer in a hospital setting could improve the diagnostic accuracy by non-expert medical staff, directly improving the health of patients suffering from pressure injuries.","technologies":["PyTorch","Python","Vision Transformers","Deep Learning","scikit-learn","pandas","numpy"]},{"name":"Predicting MBTA Reliability with Historical Data","link":"https://github.com/GrantPerkins/CS539FinalProject","image":"mbta.png","description":"The Massachusetts Bay Transportation Authority (MBTA) has a commitment to providing their customers transparent performance measures. To aid in this commitment, this project predicts whether the Red Line will be late, given just 2 weeks of historical data on the other rapid transit lines. The MBTA Reliability dataset was split into disjoint training and testing sets. An exhaustive search of methods of scaling, feature decomposition, and models was performed, as well as five-fold cross validation. This produced PCA into random forest pipeline capable of 61% accuracy on predicting if the Red Line will be late. Additional models explored include GRU, LSTM, SVM, and logistic regression.","technologies":["Python","scikit-learn"]},{"name":"Using Intermarket Data to Evaluate the Efficient Market Hypothesis with Machine Learning","link":"https://arxiv.org/abs/2212.08734","image":"asset.png","description":"In its semi-strong form, the Efficient Market Hypothesis (EMH) implies that technical analysis will not reveal any hidden statistical trends via intermarket data analysis. If technical analysis on intermarket data reveals trends which can be leveraged to significantly outperform the stock market, then the semi-strong EMH does not hold. In this work, we utilize a variety of machine learning techniques to empirically evaluate the EMH using stock market, foreign currency (Forex), international government bond, index future, and commodities future assets. We train five machine learning models on each dataset and analyze the average performance of these models for predicting the direction of future S&P 500 movement as approximated by the SPDR S&P 500 Trust ETF (SPY). From our analysis, the datasets containing bonds, index futures, and/or commodities futures data notably outperform baselines by substantial margins. Further, we find that the usage of intermarket data induce statistically significant positive impacts on the accuracy, macro F1 score, weighted F1 score, and area under receiver operating characteristic curve for a variety of models at the 95% confidence level. This provides strong empirical evidence contradicting the semi-strong EMH.","technologies":["Python","PyTorch","scikit-learn","pandas"]},{"name":"Unmasked: Reconstructing Masked Faces with Machine Learning","link":"https://github.com/UnmaskedML/UnmaskedML","image":"gan.png","description":"The accuracy of existing face detection and recognition models are greatly compromised when obstructions, like surgical masks, occlude facial features of the individual. This paper introduces a method which uses facial reconstruction as a new entry for face detection in a mask-wearing context. In the first phase of the model, an object detection network, called EfficientDet-D0, locates the position of the mask. In the second phase, a Gated Convolutional Network and SN-PatchGAN model, in a Generative Adversarial Network, work collaboratively to reconstruct the occluded region of the face. The EfficientDet-D0 model successfully detects masks with a prediction score of 0.966 mAP with an IoU threshold of 50%. The GAN was trained successfully and reconstructed the outline of a human face, but failed to reproduce detailed facial features due to time and hardware constraints.","technologies":["Python","TensorFlow","OpenCV","Docker"]},{"name":"AWS S3 Local Implementation","link":"https://github.com/GrantPerkins/GrantPerkins.github.io/blob/master/src/data/paper.pdf","image":"s3.png","description":"Developed a local implementation of Amazon Web Services S3 to serve as a new project for WPI\'s Distributed Computing Systems course. The focus of this project was to implement quorum-based policy for fault tolerance and give students more experience with the Go programming language. This implementation supports multiple nodes saving copies of the file, file versioning per node based on the system clock, and multiple clients sending read and write requests.","technologies":["Golang"]},{"name":"Axon","link":"https://docs.wpilib.org/en/latest/docs/software/wpilib-tools/axon/introduction.html","image":"hatchcover.png","description":"While working at WPILib, I lead development on Axon, a TensorFlow-backed web app for retraining COCO trained Mobilenets for use in the FIRST Robotics Competition. Axon provides a friendly interface for creating datasets, training models with live metrics, and inferencing TFLite models. There is a Docker backend that runs various TensorFlow 1.12 processes. To trigger the Docker containers to run, there is a TypeScript server. Finally, a React frontend written in TypeScript communicates to the server using GraphQL.","technologies":["Python","C++","TypeScript","TensorFlow","OpenCV","Docker","GraphQL","React"]},{"name":"GRIP","link":"https://docs.wpilib.org/en/stable/docs/software/vision-processing/grip/introduction-to-grip.html","image":"grip.png","description":"While working at WPILib, I maintained GRIP, the Graphically Represented Image Processing engine. It provides an easy-to-use GUI to develop OpenCV vision pipelines, as well as generate code in Python, C++, and Java. GRIP uses a drag-and-drop interface to implement all base OpenCV functions, such as erode, resize, and contour detection. This tool is used by thousands of high school students every year, and is one of WPILib\'s most popular tools.","technologies":["Java","C++","Python","OpenCV","Gradle"]},{"name":"Distributed Computing Systems Class Final Project","link":"https://github.com/GrantPerkins/CS4513FinalProject/blob/master/output.ipynb","image":"sagemaker.jpg","description":"For my final project of WPI\'s Distributed Computing Systems class, I developed a distributed machine learning MNIST classification solution with TensorFlow and AWS SageMaker. The MNIST dataset is split between all of the worker computers. A custom Python program was run across these workers. Each worker had a different \\"rank\\", and this rank was used to determine which slice of the dataset the worker should use. The lead worker printed metrics every 50 epochs. My Python script trained a custom convolutional neural network, calculating loss using cross entropy across all workers. The lead worker ran the optimizer","technologies":["Python","TensorFlow","AWS SageMaker","AWS S3"]},{"name":"RattleSnake","link":"https://github.com/GrantPerkins/RattleSnake","image":"rs.png","description":"I developed my own compiled language called RattleSnake. While rudimentary, RattleSnake has a fully-functional lexer, parser, and assembly code generator for the NASM assembler. By running as an executable instead of an interpreted script, RattleSnake runs substantially faster than Python, from which the style of the language is based. RattleSnake also has clearer scope definitions than Python, and a main function. RattleSnake can currently do all basic math operations (+, -, *, /).","technologies":["x86 Assembly","Python","NASM"]},{"name":"Accurate Centroid-Determining Human Body Detection","link":"https://drive.google.com/file/d/1XcTCKMBJ2OyixukSnBX50-Wzj0SVDF7a/view?usp=sharing","image":"centroid.png","description":"Accurate Centroid-Determining Human Body Detection is a project I developed for the Massachusetts State Science Fair. I developed a triple neural network system for detecting the centroid of human body in depth images. I used a sparse auto-encoder, a convolutional neural network, sliding window localization, and a Haar cascade classifier to accomplish this task.","technologies":["Python","TensorFlow","MatPlotLib","OpenCV"]}]')},59:function(e,t,i){},71:function(e,t,i){"use strict";i.r(t);var a=i(0),n=i.n(a),r=i(9),s=i.n(r),o=(i(59),i(52)),c=i(93),l=i(101),d=i(41),h=i(20),m=i(95),p=i(105),u=i(96),g=i(34),f=i(97),b=i(50),j=i.n(b),y=i(2),v=Object(c.a)((function(e){return{element:{padding:3},bullet:{height:5,paddingBottom:1},title:{textDecoration:"underline"},media:{height:0,paddingTop:"56.25%"}}}));var w=function(e){var t=v();return Object(y.jsx)(m.a,{item:!0,xs:12,children:Object(y.jsxs)(p.a,{className:t.element,children:[Object(y.jsxs)("a",{href:e.link,className:t.title,children:[Object(y.jsx)(u.a,{title:Object(y.jsx)(g.a,{variant:"h4",color:"primary",children:e.name})}),Object(y.jsx)(f.a,{className:t.media,image:"https://raw.githubusercontent.com/GrantPerkins/GrantPerkins.github.io/master/src/data/images/"+e.image,title:"image for project"})]}),Object(y.jsx)(g.a,{variant:"body1",children:e.description}),Object(y.jsx)(g.a,{variant:"h5",children:"Technologies"}),Object(y.jsx)(m.a,{container:!0,children:e.technologies.map((function(e){return Object(y.jsx)(m.a,{item:!0,xs:6,children:Object(y.jsxs)(m.a,{container:!0,children:[Object(y.jsx)(m.a,{item:!0,xs:2,children:Object(y.jsx)(j.a,{className:t.bullet})}),Object(y.jsx)(m.a,{item:!0,xs:10,children:Object(y.jsx)(g.a,{variant:"body2",children:e})})]})})}))})]})})},k=i(40);var x=function(){var e=n.a.useState([0]),t=Object(d.a)(e,2),i=t[0],a=t[1],r=n.a.useState([1]),s=Object(d.a)(r,2),o=s[0],c=s[1];return 1===i.length&&(a(function(){for(var e=[],t=0;t<h.length;t+=2)e.push(t);return e}()),c(function(){for(var e=[],t=1;t<h.length;t+=2)e.push(t);return e}())),Object(y.jsxs)("div",{children:[Object(y.jsx)(k.BrowserView,{children:Object(y.jsxs)(m.a,{container:!0,spacing:3,children:[Object(y.jsx)(m.a,{item:!0,xs:6,children:Object(y.jsx)(m.a,{container:!0,spacing:3,children:i.map((function(e){var t=h[e];return Object(y.jsx)(w,{name:t.name,link:t.link,image:t.image,description:t.description,technologies:t.technologies},e)}))})}),Object(y.jsx)(m.a,{item:!0,xs:6,children:Object(y.jsx)(m.a,{container:!0,spacing:3,children:o.map((function(e){var t=h[e];return Object(y.jsx)(w,{name:t.name,link:t.link,image:t.image,description:t.description,technologies:t.technologies},e)}))})})]})}),Object(y.jsx)(k.MobileView,{children:Object(y.jsx)(m.a,{container:!0,spacing:3,children:h.map((function(e){return Object(y.jsx)(w,{name:e.name,link:e.link,image:e.image,description:e.description,technologies:e.technologies},e.name)}))})})]})},O=i(102),S=i(103),P=i(104),T=i(98),I=i(99),C=i(100),M=i.p+"static/media/pfp.cc12bcfd.jpg",A=Object(c.a)((function(e){return{largeIcon:{width:60,height:60,color:"primary"},image:{width:100,height:100,borderRadius:"50%",border:"7px solid #30476b"},header:{paddingTop:10},card:{paddingTop:"10%",width:120,display:"flex",textAlign:"center"},fixed:{position:"fixed"}}}));var F=function(){var e=A();return Object(y.jsx)("div",{className:e.fixed,children:Object(y.jsx)(p.a,{className:e.card,children:Object(y.jsxs)(m.a,{container:!0,spacing:2,direction:"column",justifyContent:"center",alignItems:"center",children:[Object(y.jsx)(m.a,{item:!0,children:Object(y.jsx)(S.a,{title:"Me",children:Object(y.jsx)("img",{src:M,alt:"me",className:e.image})})}),Object(y.jsx)(m.a,{item:!0,children:Object(y.jsx)(S.a,{title:"GitHub",children:Object(y.jsx)(P.a,{href:"https://github.com/GrantPerkins",children:Object(y.jsx)(T.a,{className:e.largeIcon,color:"primary"})})})}),Object(y.jsx)(m.a,{item:!0,children:Object(y.jsx)(S.a,{title:"LinkedIn",children:Object(y.jsx)(P.a,{href:"https://www.linkedin.com/in/grant-perkins-35ba00170/",children:Object(y.jsx)(I.a,{className:e.largeIcon,color:"primary"})})})}),Object(y.jsx)(m.a,{item:!0,children:Object(y.jsx)(S.a,{title:"Email",children:Object(y.jsx)(P.a,{href:"mailto:gcperkins@wpi.edu",children:Object(y.jsx)(C.a,{className:e.largeIcon,color:"primary"})})})})]})})})},G=Object(o.a)({palette:{background:{default:"#30476b"},text:{primary:"#30476b"},primary:{main:"#30476b"},secondary:{main:"#ffffff"}}}),D=Object(c.a)((function(e){return{root:{padding:"2%"},title:{display:"inline-flex",justifyContent:"center",width:"100%",paddingBottom:17},portfolio:{float:"left",width:"calc(100% - 142px)"},contact:{float:"right",width:120}}}));var N=function(){var e=D();return Object(y.jsx)(l.a,{theme:G,children:Object(y.jsxs)("div",{className:e.root,children:[Object(y.jsx)(O.a,{}),Object(y.jsx)(g.a,{className:e.title,variant:"h2",color:"secondary",children:"Grant Perkins' Portfolio"}),Object(y.jsx)("div",{style:{width:"100%"},children:Object(y.jsx)("div",{className:e.portfolio,children:Object(y.jsx)(x,{})})}),Object(y.jsx)("div",{className:e.contact,children:Object(y.jsx)(F,{})})]})})},R=function(e){e&&e instanceof Function&&i.e(3).then(i.bind(null,106)).then((function(t){var i=t.getCLS,a=t.getFID,n=t.getFCP,r=t.getLCP,s=t.getTTFB;i(e),a(e),n(e),r(e),s(e)}))};s.a.render(Object(y.jsx)(n.a.StrictMode,{children:Object(y.jsx)(N,{})}),document.getElementById("root")),R()}},[[71,1,2]]]);
//# sourceMappingURL=main.cead595a.chunk.js.map